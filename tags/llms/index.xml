<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on Anish Saxena</title>
    <link>https://Anish-Saxena.github.io/tags/llms/</link>
    <description>Recent content in LLMs on Anish Saxena</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>All rights reserved 2024</copyright>
    <lastBuildDate>Sat, 27 Jul 2024 12:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://Anish-Saxena.github.io/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>High Throughput Mixture-of-Expert Serving: Intern Talk at NVIDIA</title>
      <link>https://Anish-Saxena.github.io/talk/nvidia_2024/</link>
      <pubDate>Sat, 27 Jul 2024 12:00:00 +0000</pubDate>
      
      <guid>https://Anish-Saxena.github.io/talk/nvidia_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Memory Systems for Scalable LLM Training: Intern Talk at AMD</title>
      <link>https://Anish-Saxena.github.io/talk/end_of_intern_amd/</link>
      <pubDate>Sat, 05 Aug 2023 12:00:00 +0000</pubDate>
      
      <guid>https://Anish-Saxena.github.io/talk/end_of_intern_amd/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
